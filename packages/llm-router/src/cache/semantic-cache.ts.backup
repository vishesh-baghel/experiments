/**
 * Semantic Cache - Uses embeddings to cache similar queries
 * 
 * This is the real cost saver (40-60% reduction) compared to routing (10-20%).
 * Uses OpenAI embeddings + Upstash Vector to find semantically similar cached queries.
 */

import { embed } from 'ai';
import { openai } from '@ai-sdk/openai';
import { Index } from '@upstash/vector';

export interface CacheEntry {
  query: string;
  embedding: number[];
  response: string;
  model: string;
  provider: string;
  complexity: string;
  timestamp: number;
  hits: number;
  cost: number;
}

export interface CacheStats {
  totalEntries: number;
  totalHits: number;
  hitRate: number;
  oldestEntry: number | null;
  newestEntry: number | null;
  costSaved: number;
  avgHitsPerEntry: number;
}

export class SemanticCache {
  private vectorDb: Index | null = null;
  private similarityThreshold: number;
  private maxEntries: number;
  private totalLookups: number = 0;
  private totalHits: number = 0;
  private useVector: boolean;
  private queryEmbeddingCache: Map<string, number[]> = new Map(); // Cache query embeddings in memory

  constructor(options: {
    similarityThreshold?: number;
    maxEntries?: number;
    useVector?: boolean;
  } = {}) {
    this.similarityThreshold = options.similarityThreshold ?? 0.85;
    this.maxEntries = options.maxEntries ?? 1000;
    this.useVector = options.useVector ?? true; // Default to Upstash Vector for production
    
    if (this.useVector) {
      this.initializeVector();
    }
  }

  /**
   * Initialize Redis client
   */
  private async initializeRedis(): Promise<void> {
    try {
      this.redisClient = createClient({
        url: process.env.REDIS_URL || 'redis://localhost:6379',
      });
      
      this.redisClient.on('error', (err) => 
        console.error('[SemanticCache] Redis error:', err)
      );
      
      await this.redisClient.connect();
      console.log('[SemanticCache] Redis connected for persistent semantic caching');
    } catch (error) {
      console.error('[SemanticCache] Failed to connect to Redis:', error);
      this.useRedis = false;
    }
  }

  /**
   * Get Redis client, ensuring it's connected
   */
  private async getRedis() {
    if (!this.redisClient || !this.redisClient.isOpen) {
      await this.initializeRedis();
    }
    return this.redisClient;
  }

  /**
   * Get cached response - FAST lookup only (no embedding generation)
   * Only checks exact match to avoid blocking the response
   * Semantic matching happens asynchronously in the background
   */
  async get(query: string): Promise<CacheEntry | null> {
    this.totalLookups++;

    if (!this.useRedis) {
      return null;
    }

    try {
      const redis = await this.getRedis();
      if (!redis) return null;

      // ONLY check exact match (fast: <10ms)
      const exactMatch = await redis.hGet('semantic-cache', query);
      if (exactMatch) {
        const entry: CacheEntry = JSON.parse(exactMatch);
        console.log(`[Cache] ✓ EXACT MATCH - Returning cached response`);
        entry.hits++;
        this.totalHits++;
        
        // Update hits asynchronously (don't block response)
        redis.hSet('semantic-cache', query, JSON.stringify(entry)).catch(err => 
          console.error('[Cache] Error updating hits:', err)
        );
        
        return entry;
      }

      // No exact match - return null immediately (don't block!)
      console.log(`[Cache] ✗ MISS - No exact match found`);
      return null;
    } catch (error) {
      console.error('[Cache] Error getting from cache:', error);
      return null;
    }
  }

  /**
   * Check semantic similarity in background (async, non-blocking)
   * This runs AFTER the response is sent to find similar queries
   * and can be used for analytics or future optimizations
   */
  async checkSemanticSimilarity(query: string): Promise<CacheEntry | null> {
    if (!this.useRedis) {
      return null;
    }

    try {
      const redis = await this.getRedis();
      if (!redis) return null;

      const allEntries = await redis.hGetAll('semantic-cache');
      const keys = Object.keys(allEntries);
      
      if (keys.length === 0) {
        return null;
      }

      // Get query embedding (slow: 5-10s, but non-blocking)
      const queryEmbedding = await this.getEmbedding(query);

      // Find most similar cached query
      let maxSimilarity = -1;
      let bestMatch: CacheEntry | null = null;

      for (const key of keys) {
        const entry: CacheEntry = JSON.parse(allEntries[key]);
        const similarity = cosineSimilarity(queryEmbedding, entry.embedding);
        
        if (similarity > maxSimilarity) {
          maxSimilarity = similarity;
          bestMatch = entry;
        }
      }

      if (bestMatch && maxSimilarity >= this.similarityThreshold) {
        console.log(`[Cache] Background: Found semantic match "${bestMatch.query.substring(0, 50)}..." (${(maxSimilarity * 100).toFixed(1)}%)`);
        return bestMatch;
      }

      return null;
    } catch (error) {
      console.error('[Cache] Error in semantic similarity check:', error);
      return null;
    }
  }

  /**
   * Cache a new query-response pair
   * Generates embedding asynchronously (non-blocking)
   * Stores in Redis Hash for efficient bulk operations
   */
  async set(
    query: string,
    response: string,
    model: string,
    cost: number,
    provider?: string,
    complexity?: string
  ): Promise<void> {
    if (!this.useRedis) {
      return;
    }

    // Run cache update asynchronously (don't block caller)
    this.setCacheAsync(query, response, model, cost, provider, complexity).catch(err =>
      console.error('[Cache] Failed to cache entry:', err)
    );
  }

  /**
   * Internal async method to update cache
   * Generates embedding and stores in Redis
   */
  private async setCacheAsync(
    query: string,
    response: string,
    model: string,
    cost: number,
    provider?: string,
    complexity?: string
  ): Promise<void> {
    try {
      const redis = await this.getRedis();
      if (!redis) return;

      // Check if cache is full
      const size = await redis.hLen('semantic-cache');
      if (size >= this.maxEntries) {
        // Evict oldest entry
        await this.evictOldest();
      }

      // Generate embedding (slow: 5-10s, but async so doesn't block)
      const embedding = await this.getEmbedding(query);

      const entry: CacheEntry = {
        query,
        embedding,
        response,
        model,
        provider: provider || 'openai',
        complexity: complexity || 'unknown',
        timestamp: Date.now(),
        hits: 0,
        cost,
      };

      // Store in Redis Hash
      await redis.hSet('semantic-cache', query, JSON.stringify(entry));

      console.log(`[Cache] ✓ Stored entry for: "${query.substring(0, 50)}..."`);
    } catch (error) {
      console.error('[Cache] Error in setCacheAsync:', error);
    }
  }

  /**
   * Clear all cached entries from Redis Hash
   */
  async clear(): Promise<void> {
    if (!this.useRedis) {
      return;
    }

    try {
      const redis = await this.getRedis();
      if (!redis) return;

      const size = await redis.hLen('semantic-cache');
      await redis.del('semantic-cache');
      
      this.totalLookups = 0;
      this.totalHits = 0;
      console.log(`[Cache] Cleared ${size} entries`);
    } catch (error) {
      console.error('[Cache] Error clearing cache:', error);
    }
  }

  /**
   * Get cache statistics from Redis Hash
   */
  async getStats(): Promise<CacheStats> {
    if (!this.useRedis) {
      return {
        totalEntries: 0,
        totalHits: 0,
        hitRate: 0,
        oldestEntry: null,
        newestEntry: null,
        costSaved: 0,
        avgHitsPerEntry: 0,
      };
    }

    try {
      const redis = await this.getRedis();
      if (!redis) {
        return {
          totalEntries: 0,
          totalHits: 0,
          hitRate: 0,
          oldestEntry: null,
          newestEntry: null,
          costSaved: 0,
          avgHitsPerEntry: 0,
        };
      }

      const allEntries = await redis.hGetAll('semantic-cache');
      const entries: CacheEntry[] = Object.values(allEntries).map(e => JSON.parse(e));

      const timestamps = entries.map((e) => e.timestamp);
      const totalHits = entries.reduce((sum, e) => sum + e.hits, 0);
      const costSaved = entries.reduce((sum, e) => sum + e.hits * e.cost, 0);

      return {
        totalEntries: entries.length,
        totalHits,
        hitRate: this.totalLookups > 0 ? this.totalHits / this.totalLookups : 0,
        oldestEntry: timestamps.length > 0 ? Math.min(...timestamps) : null,
        newestEntry: timestamps.length > 0 ? Math.max(...timestamps) : null,
        costSaved,
        avgHitsPerEntry: entries.length > 0 ? totalHits / entries.length : 0,
      };
    } catch (error) {
      console.error('[Cache] Error getting stats:', error);
      return {
        totalEntries: 0,
        totalHits: 0,
        hitRate: 0,
        oldestEntry: null,
        newestEntry: null,
        costSaved: 0,
        avgHitsPerEntry: 0,
      };
    }
  }

  /**
   * Get embedding for text using OpenAI
   * Uses 256 dimensions (reduced from 1536) for 6x memory savings
   * Includes retry logic for API failures
   * Caches embeddings in memory to avoid redundant API calls
   */
  private async getEmbedding(text: string): Promise<number[]> {
    // Check cache first
    const cached = this.queryEmbeddingCache.get(text);
    if (cached) {
      console.log(`[Cache] ✓ Using cached embedding for: "${text.substring(0, 50)}..."`);
      return cached;
    }

    try {
      console.log(`[Cache] Generating embedding for: "${text.substring(0, 50)}..."`);
      const { embedding } = await embed({
        model: openai.embedding('text-embedding-3-small', {
          dimensions: 256, // Reduced from 1536 for 6x memory savings
        }),
        value: text,
        maxRetries: 3, // Retry up to 3 times on transient failures
      });
      console.log(`[Cache] ✓ Embedding generated (${embedding.length} dimensions)`);
      
      // Cache the embedding
      this.queryEmbeddingCache.set(text, embedding);
      
      // Limit cache size to prevent memory issues (LRU-like)
      if (this.queryEmbeddingCache.size > 1000) {
        const firstKey = this.queryEmbeddingCache.keys().next().value;
        if (firstKey) {
          this.queryEmbeddingCache.delete(firstKey);
        }
      }
      
      return embedding;
    } catch (error) {
      console.error('[Cache] ✗ Failed to generate embedding:', error);
      // Return a zero vector as fallback - will never match semantically
      return new Array(256).fill(0);
    }
  }

  /**
   * Evict oldest cache entry from Redis Hash (LRU-like)
   */
  private async evictOldest(): Promise<void> {
    try {
      const redis = await this.getRedis();
      if (!redis) return;

      const allEntries = await redis.hGetAll('semantic-cache');
      let oldestKey: string | null = null;
      let oldestTimestamp = Infinity;

      for (const [key, value] of Object.entries(allEntries)) {
        const entry: CacheEntry = JSON.parse(value);
        if (entry.timestamp < oldestTimestamp) {
          oldestTimestamp = entry.timestamp;
          oldestKey = key;
        }
      }

      if (oldestKey) {
        await redis.hDel('semantic-cache', oldestKey);
        console.log(`[Cache] Evicted oldest entry`);
      }
    } catch (error) {
      console.error('[Cache] Error evicting oldest entry:', error);
    }
  }

  /**
   * Set similarity threshold
   */
  setSimilarityThreshold(threshold: number): void {
    if (threshold < 0 || threshold > 1) {
      throw new Error('Similarity threshold must be between 0 and 1');
    }
    this.similarityThreshold = threshold;
  }
}
